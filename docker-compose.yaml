networks:
  app-network:
    driver: bridge
    external: true

services:

  # litellm:
  #   image: ghcr.io/berriai/litellm:latest # Or build from a Dockerfile
  #   ports:
  #     - "4001:4000"
  #   volumes:
  #     - ./litellm_config.yaml:/app/config.yaml # Mount your config file
  #   environment:
  #     - LITELLM_MASTER_KEY=${LITELLM_MASTER_KEY}
  #     - LITELLM_SALT_KEY=${LITELLM_SALT_KEY}

  #     - AZURE_OPENAI_API_KEY=""
  #     - AZURE_OPENAI_ENDPOINT=""
  #     - AZURE_OPENAI_API_VERSION=""
  #     - AZURE_API_BASE=""
  #     - AZURE_OPENAI_AD_TOKEN=""

  #     # Add other LLM API keys as needed
  #     # - OPENAI_API_KEY=${OPENAI_API_KEY}
  #   command: ["litellm", "--config", "/app/config.yaml", "--port", "4000"]

 

  pg_db_ai:
    image: postgres:18.0-alpine
    container_name: pg_db_ai
    hostname: pg_db_ai
    environment:
      - POSTGRES_USER=transcription
      - POSTGRES_PASSWORD=fK7-SEqRwMsSREpAHBnU
      - POSTGRES_DB=openwebui
    volumes:
      - ./data/pgdata/:/var/lib/postgresql/
      # - ./data/pgdata-latest:/var/lib/postgresql
    ports:
      - "5433:5432"
    networks:
      - app-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U transcription -d openwebui"]
      interval: 10s
      timeout: 5s
      retries: 5

  # grafana:
  #   image: grafana/grafana-oss
  #   container_name: grafana
  #   hostname: grafana
  #   ports:
  #     - "3008:3000"
  #   environment:
  #     - GF_SECURITY_ADMIN_USER=admin
  #     - GF_SECURITY_ADMIN_PASSWORD=supersecure123
  #   volumes:
  #     - ./data/grafana-storage:/var/lib/grafana
  #   extra_hosts:
  #     - "host.docker.internal:host-gateway"
  #   restart: always
  #   networks:
  #     - app-network
 


  ollama:
    volumes:
      - ./data/ollama:/root/.ollama
    container_name: ollama
    # pull_policy: always
    tty: true
    restart: unless-stopped
    image: ollama/ollama:${OLLAMA_DOCKER_TAG-latest}
    networks:
      - app-network

  open-webui:
    build:
      context: .
      args:
        OLLAMA_BASE_URL: '/ollama'
      dockerfile: Dockerfile
    image: ghcr.io/open-webui/open-webui:${WEBUI_DOCKER_TAG-main}
    container_name: open-webui
    env_file:
      - .env
    volumes:
      - .:/app # <â€” edit locally, container runs your code
      # - ./webui-data:/app/backend/data      # persistent app data (users, settings, etc.)
      - ./webui-cache:/root/.cache          # optional cache (models, etc.)
    # # volumes:
      - open-webui:/app/backend/data
    # volumes:
    #   - ./open-webui-data:/app/backend/data
    #   - ./backend:/app/backend 
    #   - ./build:/app/build
    #   - ./CHANGELOG.md:/app/CHANGELOG.md
    #   # - ./package.jso/n:/app/package.json
    #   # - ./.env:/app/.env
      - ./static/static:/app/build/static
    #   # - ./:/app/

    depends_on:
      - ollama
      - pg_db_ai
      - redis
    ports:
      # - ${OPEN_WEBUI_PORT-3000}:8080
      - "${WEBUI_PORT:-8080}:8080"
    environment:
      - 'ENV=${ENV:-prod}'
      - 'WEBUI_NAME=${WEBUI_NAME:-AI Legal Assistant}'
      - 'WEBUI_PORT=${WEBUI_PORT}'
      - 'BASE_URL=${WEBUI_URL:-https://lexluma.com}'
      - 'CORS_ALLOW_ORIGIN=${CORS_ALLOW_ORIGIN}'
      - 'WEBUI_URL=${WEBUI_URL:-https://lexluma.com}'
      - 'ENABLE_OAUTH_PERSISTENT_CONFIG=false'     # so env changes apply during testing
      - 'OLLAMA_BASE_URL=${OLLAMA_BASE_URL}'  #
      - 'OLLAMA_API_KEY=${OLLAMA_API_KEY}'  #
      - 'WEBUI_SECRET_KEY=${SECRET_KEY}'  # set in .env file
      # - 'ENABLE_OTEL_METRICS=false'
      # - 'OTEL_EXPORTER_OTLP_INSECURE=true' # Use insecure connection for OTLP, remove in production
      # - 'OTEL_EXPORTER_OTLP_ENDPOINT=http://grafana:4317'
      # - 'OTEL_SERVICE_NAME=open-webui'
      - 'DATABASE_URL=${DATABASE_URL:-postgresql://transcription:fK7-SEqRwMsSREpAHBnU@pg_db_ai/webui}'
      # - 'VECTOR_DB=pgvector'

      - USER_AGENT=${USER_AGENT:-Open WebUI (https://openwebui.ai)}
      - ENABLE_WEBSOCKET_SUPPORT="true"
      - WEBSOCKET_MANAGER="redis"
      - WEBSOCKET_REDIS_URL="redis://redis:6379/1"
      - REDIS_KEY_PREFIX="open-webui"
      - ENABLE_OAUTH_SIGNUP=${ENABLE_OAUTH_SIGNUP:-True}
      - ENABLE_SIGNUP_PASSWORD_CONFIRMATION=${ENABLE_SIGNUP_PASSWORD_CONFIRMATION:-True}
      - ENABLE_SIGNUP=True
      - ENABLE_CHANNELS=${ENABLE_CHANNELS:-True}
      - ENABLE_AUDIT_LOGGING=${ENABLE_AUDIT_LOGGING:-True}
      - DEFAULT_PROMPT_SUGGESTIONS=${DEFAULT_PROMPT_SUGGESTIONS:-[]}
      - WEBUI_BANNERS=${WEBUI_BANNERS:-[]}
      - ENABLE_VERSION_UPDATE_CHECK=${ENABLE_VERSION_UPDATE_CHECK:-True} 
      - ENABLE_COMMUNITY_SHARING=${ENABLE_COMMUNITY_SHARING:-True}
      - GLOBAL_LOG_LEVEL=${GLOBAL_LOG_LEVEL:-DEBUG}

    
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://localhost:8080/"]
      interval: 15s
      timeout: 5s
      retries: 20
    ulimits:
      nofile:
        soft: 65535
        hard: 65535
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
    
    extra_hosts:
      - host.docker.internal:host-gateway
    restart: unless-stopped
    networks:
      - app-network

  redis:
    image: valkey/valkey:9.0-alpine3.22
    container_name: redis-valkey
    hostname: redis-valkey
    volumes:
      - ./data/redis-data:/data
    command: "valkey-server --save 30 1"
    healthcheck:
      test: "[ $$(valkey-cli ping) = 'PONG' ]"
      start_period: 5s
      interval: 1s
      timeout: 3s
      retries: 5
    restart: unless-stopped
    cap_drop:
      - ALL
    cap_add:
      - SETGID
      - SETUID
      - DAC_OVERRIDE
    logging:
      driver: "json-file"
      options:
        max-size: "1m"
        max-file: "1"
    networks:
      - app-network

volumes:
  open-webui: {}
  pgdata: {}