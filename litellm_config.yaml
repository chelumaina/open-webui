# litellm_config.yaml â€” Ollama Cloud only

environment_variables:
  # hard-disable azure so litellm / openai lib can't pick it up
  AZURE_OPENAI_API_KEY: ""
  AZURE_OPENAI_ENDPOINT: ""
  AZURE_OPENAI_API_VERSION: ""
  AZURE_API_BASE: ""
  AZURE_OPENAI_AD_TOKEN: ""

general_settings:
  master_key: os.environ/LITELLM_MASTER_KEY
  database_url: os.environ/DATABASE_URL
  store_prompts_in_spend_logs: true

router_settings:
  num_retries: 2
  timeout: 60
  routing_strategy: simple-shuffle

litellm_settings:
  default_key_generate_params:
    models: ["basic-models"]
    max_budget: 10.0
    budget_duration: "30d"
    rpm_limit: 30
    tpm_limit: 60000
    tags: ["plan:basic"]
  upperbound_key_generate_params:
    max_budget: 500
    budget_duration: "90d"
    rpm_limit: 600
    tpm_limit: 1000000

model_list:
  # ===================== BASIC =====================
  - model_name: basic-chat
    litellm_params:
      # the important parts ðŸ‘‡
      custom_llm_provider: ollama
      model: llama3.1:8b
      api_base: os.environ/OLLAMA_API_BASE_CLOUD  # e.g. https://ollama.com/api
      keep_alive: "8m"
      temperature: 0.4
    model_info:
      access_groups: ["basic-models", "Admin"]
      description: "Basic â€“ Llama 3.1 8B (Ollama Cloud)"
      input_cost_per_1m_token: 0.30
      output_cost_per_1m_token: 1.50
      input_cost_per_token: 3e-7
      output_cost_per_token: 1.5e-6
      max_tokens: 128000

  # ===================== ENTERPRISE =====================
  - model_name: enterprise-chat
    litellm_params:
      custom_llm_provider: ollama
      model: llama3.1:70b
      api_base: os.environ/OLLAMA_API_BASE_CLOUD
      keep_alive: "8m"
    model_info:
      access_groups: ["enterprise-models", "Admin"]
      description: "Enterprise â€“ Llama 3.1 70B (Ollama Cloud)"
      input_cost_per_1m_token: 1.50
      output_cost_per_1m_token: 6.00
      input_cost_per_token: 1.5e-6
      output_cost_per_token: 6e-6
      max_tokens: 128000

  # ===================== ENTERPRISE PLUS =====================
  - model_name: enterprise-plus-chat
    litellm_params:
      custom_llm_provider: ollama
      model: mixtral:8x22b
      api_base: os.environ/OLLAMA_API_BASE_CLOUD
      keep_alive: "8m"
    model_info:
      access_groups: ["enterprise-plus-models", "Admin"]
      description: "Enterprise Plus â€“ Mixtral 8x22B (Ollama Cloud)"
      input_cost_per_1m_token: 3.00
      output_cost_per_1m_token: 12.00
      input_cost_per_token: 3e-6
      output_cost_per_token: 1.2e-5
      max_tokens: 128000
