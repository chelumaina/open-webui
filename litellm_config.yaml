general_settings:
  litellm_port: 4000
  mode: proxy  # ensures it runs in proxy mode

model_list:
  - model_name: gpt-3.5-turbo
    litellm_provider: openai
    api_key: your_openai_key_here

  - model_name: local-llm
    litellm_provider: ollama
    api_base: http://host.docker.internal:11434
    model_info:
      mode: chat
